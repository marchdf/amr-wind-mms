#!/bin/bash

#SBATCH --job-name=amr-wind-mms
#SBATCH --account=hfm
#SBATCH --nodes=1
#SBATCH --time=0:10:00
#SBATCH -o %x.o%j

module purge
source /projects/hfm/shreyas/exawind/scripts/exawind-env-gcc.sh

ranks_per_node=36
mpi_ranks=$(expr $SLURM_JOB_NUM_NODES \* $ranks_per_node)
export OMP_NUM_THREADS=1  # Max hardware threads = 4
export OMP_PLACES=threads
export OMP_PROC_BIND=spread

amrwind_exec=$HOME/exawind/source/amr-wind/build/amr_wind

echo "Job name       = $SLURM_JOB_NAME"
echo "Num. nodes     = $SLURM_JOB_NUM_NODES"
echo "Num. MPI Ranks = $mpi_ranks"
echo "Num. threads   = $OMP_NUM_THREADS"
echo "Working dir    = $PWD"

cp ${amrwind_exec} $(pwd)/amr_wind

cells=( 8 16 32 64 )
for cell in "${cells[@]}"
do
    rm -rf $cell
    mkdir $cell
    cd $cell || exit
    cp ../mms.i .
    # srun -n 1 --gres=gpu:2 ../amr_wind mms.i amr.n_cell=$cell $cell $cell >> mms.o
    srun -n ${mpi_ranks} -c 1 --cpu_bind=cores ../amr_wind mms.i amr.n_cell=$cell $cell $cell >> mms.o
    ls -1v *plt*/Header | tee movie.visit
    cd .. || exit
done
